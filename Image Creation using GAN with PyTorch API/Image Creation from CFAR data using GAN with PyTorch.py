# Importing the libraries
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.optim as optim
import torch.uitls.data
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torch.autograd import Variable
import torchvision.utils as utils

# Imporing the defined user defined class
from generator import G
from discriminator import D

# Setting the hyperparameters
batch_size = 64
image_size = 64 # 64 X 64 -> size of the image being generated

# Creating the transformation to make the input image compatible witht he neural net of the generator
transform = transforms.Compose([transforms.Sccale(image_size), # scaling
                                transforms.ToTensor(), # tensor conversion
                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # notmalizATION
                               ])

# Loading the data
data = dset.CIFAR10(root = './data', download = True, transform = transform)
# downloading the dataset into the 'data' folder and applying the above defined trasformation on each image
dataloader = torch.utils.data.DataLoader(data, 
                                         batch_size = batch_size, # using dataLoader to get the images of the training set batch by batch
                                         shuffle = True, # to read the dats in random order
                                         num_workers = 2 # 2 parallel threads to load the data
                                        )

# Defining the weight initialization function that takes a neural net as an inout and initializez all its weights
def weights_init(n):
    classname = n.__class__.__name__
    if classname.find('Conv') != -1: # for convolutional layers
        n.weight.data.normal_(0.0, 0.02) # drawn from a normal distribution centered around 0 with a std 0.02
    elif classname.find('BatchNorm') != -1: # for BatchNorm layers
        n.weight.data.normal_(1.0, 0.02)
        '''
        By using a normal distribution centered around 1 with a std of 0.02 all the desirable effects of the batch
        normalization are preserved (hidden layer output dot product with a matrix of ones) adding on top of that 
        some extra noise, noise which helps with the generalization power of the model: it allows the model to recognize
        features not necessarily present in the training set
        '''
        n.bias.data.fill_(0)

# Creating the generator
netG = G() # creating 'Generator' object
netG.apply(weights_init) # initializing the weights for the 'Generator'

# Creating the discriminator
netD = D() # creating 'Discriminator' object
netD.apply(weights_init) # initializing the weights for the 'Discriminator'

# Defining the loss and optimizer
criterion = nn.BCELoss() 
# Binary Cross Entropy loss(measure of error between the prediction and the target.
# Measure of how close the generated image from the generator is to the actual image)

# using 'Adam' optimizer to optimize stochastic gradient descent
# optimizer for the generator
optimizerG = optim.Adam(netG.parameters(), 
                        lr = 0.0002, # learning rate
                        betas = (0.5, 0.999) # coefficients used for computing running averages of gradient and its square
                        )
# optimizer for the discriminator
optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999)) 


# Training the Deep Convolutional GANs(DCGAN)
epochs = 50
for epoch in range(epochs):
    for i, data in enumerate(dataloader, 0): # iterating thru all the images in the dataset one batch at a time
        
        # ----------------- STEP 1: Updating the weights of the neural network of the discriminator -----------------
        
        netD.zero_grad() # initializing the gradients of the discriminator with respect to the weights

        # Training the discriminator with a real image
        real, _ = data # each batch at a time(we only care about the data.so we can ignore the labels)
        input = Variable(real) # converting the data into paytorch format
        target = Varibale(torch.ones(input.size()[0]))
        # target of the discriminator(in case of real images) is always the 1s. input.size()[0] gives the length of the batch
        output = netD(input) # propagating the a batch of real images thru the discriminator network
        errD_real = criterion(output, target) # Measure of how close the generated image is to the actual image
        
        
        # Training the discriminator with a fake image generated by the generator
        '''
        We want to propagate randon noise of size 100 thru to genrator to generate fake images
        ''' 
        noise = Variable(torch.randn(input.size()[0]), # random noise as input
                         100, # size of the input noise
                         1, 1 # 100 features of size 1 X 1(to have feature maps in matrix form)
                        )
        fake = netG(noise) # propagating the noise thru the generator network
        target = Varibale(torch.zeros(input.size()[0]))
        '''
        Target of the discriminator(in case of real images) is always the 0s. input.size()[0] gives the length of the batch.
        The intension of the genrator is always to trick the discriminator. But the discriminator shopuld always intend to 
        find the diffenece between the geenrated value(fake) and the real value. Hence for the fake inages, the discriminator 
        should always identigy the output as 0.
        
        '''
        output = netD(fake.detach) # propagating the fake images thru the discriminator network
        # detaching the gradient(we only use the tensor of the prediction) 
        errD_fake = criterion(output, target) 
        
        
        # Backpropagating the total error
        '''
        Target of the discriminator is always 1(i.e. we always wan the output of the generator to match with the actual image). 
        But we will propagate the error(actual ouput of the discriminator and 1) 

        Computing the total error of the discriminator, then backpropagating the loss error y computing the gradients of 
        the total error with respect to the weights of the discriminator

        '''
        errD = errD_real + errD_fake # total error
        errD.backward() # backpropagating the error thru discriminator
        optimizerD.step() # Applying the optimizer to update the weights according to how much they are
        # responsible for the loss error of the discriminator
        
        
        # ----------------- STEP 2: Updating the weights of the neural network of the generator -----------------
        
        netG.zero_grad() # initializing the gradients of the generator with respect to teh weights
        target = Varibale(torch.ones(input.size()[0]))
        # Here the generator tries to fool the discriminator and convince that the fake image is real one.
        output = netD(fake) # here we wna tot keeo thge gradient of the fake. We need the gradient to update the weights.
        # hence not using detach
        # We forward propagate the fake generated images into the discriminator to get the prediction (a value between 0 and 1)
        errG = criterion(output, target) # Measure of how close the prediction(between 0 and 1) is from target(always 1)
        errG.backward() # backpropagating the error thru generator
        optimizerG.step() # Applying the optimizer to update the weights according to how much they are
        # responsible for the loss error of the generator
        
        # Printong the losses of the discriminator and the generator 
        print('[Epoch %d of %d epochs][Step %d of total %d steps] Loss_D: %.4f Loss_G: %.4f'
              %(epoch, epochs, i, len(dataloader), errD.data[0], errG.data[0]))
        
        # Saving the real images and the generated images of the batch every 100 steps
        if i%100 == 0:
            utils.save_image(real, './Results/real_samples.png', normalize = True) # saving real images of each batch
            fake = netG(noise) # generator generated image based on random noise input
            utils.save_image(fake.data, './Results/generated_samples_%03d.png'%epoch, normalize = True) 
            # saving faek images of each batch after each epoch        